{"index_struct": {"__type__": "list", "__data__": {"index_id": "73dd8a6c-d396-498c-8cfd-62a56c4daf08", "summary": null, "nodes": ["dc6dfdfb-e39e-44b6-ac60-fc0cc21cf377", "e7033e90-60f5-4941-a7f0-1fb1dea05e60", "71488cdf-da26-4d3e-b2e4-5d77797ec993", "c7c63ae5-7725-4b1f-9fcc-62b5991a609b", "8ce0d0ef-4e9b-4d84-a745-9abd388b991f", "f207099d-ecea-402a-874e-63874d453c3c", "7a6828b8-65d3-4c6b-ac63-b97e93aa994e"]}}, "docstore": {"docs": {"dc6dfdfb-e39e-44b6-ac60-fc0cc21cf377": {"text": "1764.1 Pr ocesses and Threads\nMultithreading\nThread Functionality\n4.2\n T\nypes of Threads\nUser-Level and Kernel-Level ThreadsOther Arrangements\n4.3\n Multicor\ne and Multithreading\nPerformance of Software on MulticoreApplication Example: Valve Game Software\n4.4\n W\nindows Process and Thread Management\nManagement of Background Tasks and Application LifecyclesThe Windows ProcessProcess and Thread ObjectsMultithreadingThread StatesSupport for OS Subsystems\n4.5\n Solaris \nThread and SMP Management\nMultithreaded ArchitectureMotivationProcess StructureThread ExecutionInterrupts as Threads\n4.6\n Linux Pr\nocess and Thread Management\nLinux TasksLinux ThreadsLinux Namespaces\n4.7\n Andr\noid Process and Thread Management\nAndroid ApplicationsActivitiesProcesses and Threads\n4.8\n Mac OS X Grand Central Dispatch\n4.\n9\n Summary\n4.\n10\n K\ney Terms, Review Questions, and ProblemsThreads Chapter \nM04_STAL4290_09_GE_C04.indd   176 5/2/17   4:38 PM\n4.1 / PROCESSES AND THREADS   177\nThis chapter examines some more advanced concepts related to process manage-\nment, which are found in a number of contemporary operating systems. We show that the concept of process is more complex and subtle than presented so far and in fact embodies two separate and potentially independent concepts: one relat-ing to resource ownership, and another relating to execution. This distinction has led to the development, in many operating systems, of a construct known as the thread.\n 4. 1 PR OCESSES AND THREADS\nThe discussion so far has presented the concept of a process as embodying two characteristics:\n1.\n R\nesource ownership: A process includes a virtual address space to hold the \nprocess image;\u00a0recall from Chapter 3 that the process image is the collection of program, data, stack, and attributes defined in the process control block. From time to time, a process may be allocated control or ownership of resources, such as main memory, I/O channels, I/O devices, and files. The OS performs a protection function to prevent unwanted interference between processes with respect to resources.\n2.\n Scheduling/e\nxecution: The execution of a process follows an execution path \n(trace) through one or more programs \u00a0(e.g., Figure 1.5). This execution may \nbe interleaved with that of other processes. Thus, a process has an execution state (Running, Ready, etc.) and a dispatching priority, and is the entity that is scheduled and dispatched by the OS.\nSome thought should convince the reader that these two characteristics are \nindependent and could be treated independently by the OS. This is done in a number \nof operating systems, particularly recently developed systems. To distinguish the two characteristics, the unit of dispatching is usually referred to as a thread or Learning  Objectives\nAfter studying this chapter, you should be able to:\n\u2022\tUnderstand the distinction between process and thread.\n\u2022\tDescribe the basic design issues for threads.\n\u2022\tExplain the difference between user-level threads and kernel-level threads.\n\u2022\tDescribe the thread management facility in Windows.\n\u2022\tDescribe the thread management facility in Solaris.\n\u2022\tDescribe the thread management facility in Linux.\nM04_STAL4290_09_GE_C04.indd   177 5/2/17   4:38 PM\n178  CHAPTER  4 / T HREADS\nlightweight\u00a0process, while the unit of resource ownership is usually referred to as a \nprocess or task.1\nMultithreading\nMultithreading refers to the ability of an OS to support multiple, concurrent paths of execution within a single process. The traditional approach of a single thread of execution per process, in which the concept of a thread is not recognized, is referred to as a single-threaded approach. The two arrangements shown in the left half of Figure 4.1 are single-threaded approaches. MS-DOS is an example of an OS that supports a single-user process and a single thread. Other operating systems, such as some variants of UNIX, support multiple user processes, but only support one thread per process. The right half of Figure 4.1 depicts multithreaded approaches. A\u00a0Java runtime environment is an example of a system of one process with multiple threads. Of interest in this section is the use of multiple processes, each of which supports multiple threads. This approach is taken in Windows, Solaris, and many \n1Alas, even this degree of consistency is not maintained. In IBM\u2019s mainframe operating systems, the con-\ncepts of address space and task, respectively, correspond roughly to the concepts of process and thread that \nwe describe in this section. Also, in the literature, the term lightweight process is used as either (1)\u00a0equiva-lent to the term thread, (2) a particular type of thread known as a kernel-level thread, or (3) in the case of Solaris, an entity that maps user-level threads to kernel-level threads.Figure 4.1  Thr eads and ProcessesOne process\nOne threadOne process\nMultiple threads\nMultiple processes\nOne thread per process\n= Instruction traceMultiple processes\nMultiple threads per process\nM04_STAL4290_09_GE_C04.indd   178 5/2/17   4:38 PM\n4.1 / PROCESSES AND THREADS   179\nmodern\u00a0versions of UNIX, among others. In this section, we give a general descrip-\ntion of multithreading; the details of the Windows, Solaris, and Linux approaches will be discussed later in this chapter.\nIn a multithreaded environment, a process is defined as the unit of resource \nallocation and a unit of protection. The following are associated with processes:\n\u2022\tA virtual address space that holds the process image\n\u2022\tProtected access to processors, other processes (for interprocess communica-tion), files, and I/O resources (devices and channels)\nWithin a process, there may be one or more threads, each with the following:\n\u2022\tA thread execution state (Running, Ready, etc.)\n\u2022\tA saved thread context when not running; one way to view a thread is as an independent program counter operating within a process\n\u2022\tAn execution stack\n\u2022\tSome per-thread static storage for local variables\n\u2022\tAccess to the memory and resources of its process, shared with all other threads in that process\nFigure 4.2 illustrates the distinction between threads and processes from the \npoint of view of process management. In a single-threaded process model (i.e., there \nis no distinct concept of thread), the representation of a process includes its process control block and user address space, as well as user and kernel stacks to manage the call/return behavior of the execution of the process. While the process is running, it controls the processor registers. The contents of these registers are saved when the process is not running. In a multithreaded environment, there is still a single process \nFigure 4.2  Single-Thr eaded and Multithreaded Process ModelsSingle-threaded\nprocess model\nProcess\ncontrol\nblock\nUser\naddress\nspaceUser\nstack\nKernel\nstackMultithreaded\nprocess model\nProcess\ncontrol\nblock\nUser\naddress\nspaceUser\nstack\nKernel\nstackUser\nstack\nKernel\nstackUser\nstack\nKernel\nstackThread\ncontrol\nblockThread Thread Thread\nThread\ncontrol\nblockThread\ncontrol\nblock\nM04_STAL4290_09_GE_C04.indd   179 5/2/17   4:38 PM\n180  CHAPTER  4 / T HREADS\ncontrol block and user address space associated with the process, but now there are \nseparate stacks for each thread, as well as a separate control block for each thread containing register values, priority, and other thread-related state information.\nThus, all of the threads of a process share the state and resources of that process. \nThey reside in the same address space and have access to the same data. When one thread alters an item of data in memory, other threads see the results if and when they access that item. If one thread opens a file with read privileges, other threads in the same process can also read from that file.\nThe key benefits of threads derive from the performance implications:\n1.\n It\n takes far less time to create a new thread in an existing process, than to create \na brand-new process. Studies done by the Mach developers show that thread creation is ten times faster than process creation in UNIX [TEVA87].\n2.\n It tak\nes less time to terminate a thread than a process.\n3.\n It tak\nes less time to switch between two threads within the same process than \nto switch between processes.\n4.\n T\nhreads enhance efficiency in communication between different executing \nprograms. In most operating systems, communication between independent processes requires the intervention of the kernel to provide protection and the mechanisms needed for communication. However, because threads within the same process share memory and files, they can communicate with each other without invoking the kernel.\nThus, if there is an application or function that should be implemented as a set \nof related units of execution, it is far more efficient to do so as a collection of threads, \nrather than a collection of separate processes.\nAn example of an application that could make use of threads is a file server. As \neach new file request comes in, a new thread can be spawned for the file management program. Because a server will handle many requests, many threads will be created and destroyed in a short period. If the server runs on a multiprocessor computer, then multiple threads within the same process can be executing simultaneously on different processors. Further, because processes or threads in a file server must share file data and therefore coordinate their actions, it is faster to use threads and shared memory than processes and message passing for this coordination.\nThe thread construct is also useful on a single processor to simplify the structure \nof a program that is logically doing several different functions.\n[LETW88] gives four examples of the uses of threads in a single-user multipro-\ncessing system:\n1.\n F\noreground and background work: For example, in a spreadsheet program, one \nthread could display menus and read user input, while another thread executes user commands and updates the spreadsheet. This arrangement often increases the perceived speed of the application by allowing the program to prompt for the next command before the previous command is complete.\n2.\n Asynchr\nonous processing: Asynchronous elements in the program can be \nimplemented as threads. For example, as a protection against power failure, one can design a word processor to write its random access memory (RAM) \nM04_STAL4290_09_GE_C04.indd   180 5/2/17   4:38 PM\n4.1 / PROCESSES AND THREADS   181\nbuffer to disk once every minute. A thread can be created whose sole job is \nperiodic backup and that schedules itself directly with the OS; there is no need for fancy code in the main program to provide for time checks or to coordinate input and output.\n3.\n Speed of e\nxecution: A multithreaded process can compute one batch of data \nwhile reading the next batch from a device. On a multiprocessor system, mul-tiple threads from the same process may be able to execute simultaneously. Thus, even though one thread may be blocked for an I/O operation to read in a batch of data, another thread may be executing.\n4.\n Modular pr\nogram structure: Programs that involve a variety of activities or a \nvariety of sources and destinations of input and output may be easier to design and implement using threads.\nIn an OS that supports threads, scheduling and dispatching is done on a thread \nbasis; hence, most of the state information dealing with execution is maintained \nin thread-level data structures. There are, however, several actions that affect all of the threads in a process, and that the OS must manage at the process level. For example, suspension involves swapping the address space of one process out of main memory to make room for the address space of another process. Because all threads in a process share the same address space, all threads are suspended at the same time. Similarly, termination of a process terminates all threads within that process.\nThread Functionality\nLike processes, threads have execution states and may synchronize with one another. We look at these two aspects of thread functionality in turn.\nThread  STaTeS  As with processes, the key states for a thread are Running, Ready, \nand Blocked. Generally, it does not make sense to associate suspend states with threads because such states are process-level concepts. In particular, if a process is swapped out, all of its threads are necessarily swapped out because they all share the address space of the process.\nThere are four basic thread operations associated with a change in thread state \n[ANDE04]:\n1.\n Spa\nwn: Typically, when a new process is spawned, a thread for that process \nis also spawned. Subsequently, a thread within a process may spawn another thread within the same process, providing an instruction pointer and arguments for the new thread. The new thread is provided with its own register context and stack space and placed on the Ready queue.\n2.\n Block:\n When a thread needs to wait for an event, it will block (saving its user \nregisters, program counter, and stack pointers). The processor may then turn to the execution of another ready thread in the same or a different process.\n3.\n Unblock:\n When the event for which a thread is blocked occurs, the thread is \nmoved to the Ready queue.\n4.\n Finish:\n When a thread completes, its register context and stacks are deallocated.\nM04_STAL4290_09_GE_C04.indd   181 5/2/17   4:38 PM\n182  CHAPTER  4 / T HREADS\nA significant issue is whether the blocking of a thread results in the blocking \nof the entire process. In other words, if one thread in a process is blocked, does this \nprevent the running of any other thread in the same process, even if that other thread is in a ready state? Clearly, some of the flexibility and power of threads is lost if the one blocked thread blocks an entire process.\nWe will return to this issue subsequently in our discussion of user-level versus \nkernel-level threads, but for now, let us consider the performance benefits of threads that do not block an entire process. Figure 4.3 (based on one in [KLEI96]) shows a program that performs two remote procedure calls (RPCs)\n2 to two different hosts to \nobtain a combined result. In a single-threaded program, the results are obtained in sequence, so the program has to wait for a response from each server in turn. Rewrit-ing the program to use a separate thread for each RPC results in a substantial speedup. Note if this program operates on a uniprocessor, the requests must be gener -\nated sequentially and the results processed in sequence; however, the program waits concurrently for the two replies.\n2An RPC is a technique by which two programs, which may execute on different machines, interact using \nprocedure call/return syntax and semantics. Both the called and calling programs behave as if the partner \nprogram were running on the same machine. RPCs are often used for client/server applications\u00a0and will be discussed in Chapter 16 .Figure 4.3  R emote Procedure Call (RPC) Using Threads(a) RPC using single thread\n(b) RPC using one thread per server (on a uniprocessor)Time\nProcess 1\nBlocked, waiting for response to RPC\nBlocked, waiting for processor, which is in use by Thread B\nRunningThread A (Process 1)\nThread B (Process 1)Server Server\nServer\nServerRPC\nrequest\nRPC\nrequest\nRPC\nrequestRPC\nrequest\nM04_STAL4290_09_GE_C04.indd   182 5/2/17   4:38 PM\n4.2 / TYPES OF THREADS   183\nOn a uniprocessor, multiprogramming enables the interleaving of multiple \nthreads within multiple processes. In the example of Figure 4.4, three threads in two \nprocesses are interleaved on the processor. Execution passes from one thread to another either when the currently running thread is blocked or when its time slice is exhausted.\n3\nThread  SynchronizaTion  All of the threads of a process share the same address \nspace and other resources, such as open files. Any alteration of a resource by one thread affects the environment of the other threads in the same process. It is therefore necessary to synchronize", "doc_id": "dc6dfdfb-e39e-44b6-ac60-fc0cc21cf377", "embedding": null, "doc_hash": "fa7e7645fb822fd089cb37009c93e2a9fd51d2287f357050d44d236563da231b", "extra_info": null, "node_info": {"start": 0, "end": 15974}, "relationships": {"1": "46f5ffc1-aec9-49dc-8b66-426492df7e64", "3": "e7033e90-60f5-4941-a7f0-1fb1dea05e60"}, "__type__": "1"}, "e7033e90-60f5-4941-a7f0-1fb1dea05e60": {"text": "environment of the other threads in the same process. It is therefore necessary to synchronize the activities of the various threads so that they do not interfere with each other or corrupt data structures. For example, if two threads each try to add an element to a doubly linked list at the same time, one element may be lost or the list may end up malformed.\nThe issues raised and the techniques used in the synchronization of threads \nare, in general, the same as for the synchronization of processes.\u00a0These issues and techniques will be the subject of Chapters 5 and 6.\n 4. 2 TYPES OF THREADS\nUser-Level and Kernel-Level Threads\nThere are two broad categories of thread implementation: user-level threads (ULTs) and kernel-level threads (KLTs).\n4 The latter are also referred to in the literature as \nkernel-supported threads or lightweight processes.\nUSer-Leve L Thread S In a pure ULT facility, all of the work of thread management \nis done by the application and the kernel is not aware of the existence of threads. \n3In this example, thread C begins to run after thread A exhausts its time quantum, even though thread B \nis also ready to run. The choice between B and C is a scheduling decision, a topic covered in Part Four.\n4The acronyms ULT and KLT are not widely used, but are introduced for conciseness.Figure 4.4  Multithr eading Example on a UniprocessorTime\nBlockedI/O\nrequest\nThread A (Process 1)\nThread B (Process 1)\nThread C (Process 2)\nReady RunningRequest\ncompleteTime quantum\nexpires\nTime quantum\nexpires\nProcess\ncreated\nM04_STAL4290_09_GE_C04.indd   183 5/2/17   4:38 PM\n184  CHAPTER  4 / T HREADS\nFigure 4.5a illustrates the pure ULT approach. Any application can be programmed \nto be multithreaded by using a threads library, which is a package of routines for ULT management. The threads library contains code for creating and destroying threads, for passing messages and data between threads, for scheduling thread execution, and for saving and restoring thread contexts.\nBy default, an application begins with a single thread and begins running in \nthat thread. This application and its thread are allocated to a single process man-aged by the kernel. At any time that the application is running (the process is in the Running state), the application may spawn a new thread to run within the same process. Spawning is done by invoking the spawn utility in the threads library. Con-trol is passed to that utility by a procedure call. The threads library creates a data structure for the new thread and then passes control to one of the threads within this process that is in the Ready state, using some scheduling algorithm. When control is passed to the library, the context of the current thread is saved, and when control is passed from the library to a thread, the context of that thread is restored. The context essentially consists of the contents of user registers, the program counter, and stack pointers.\nAll of the activity described in the preceding paragraph takes place in user \nspace and within a single process. The kernel is unaware of this activity. The ker -\nnel continues to schedule the process as a unit and assigns a single execution state (Ready, Running, Blocked, etc.) to that process. The following examples should clarify the relationship between thread scheduling and process scheduling. Suppose process B is executing in its thread 2; the states of the process and two ULTs that are part of the process are shown in Figure 4.6a. Each of the following is a possible occurrence:\n1.\n T\nhe application executing in thread 2 makes a system call that blocks B. For \nexample, an I/O call is made. This causes control to transfer to the kernel. The kernel invokes the I/O action, places process B in the Blocked state, and Figure 4.5  User -Level and Kernel-Level ThreadsP PUser\nspaceThreads\nlibrary\nKernel\nspace\nP\nPUser\nspace\nKernel\nspace\nPUser\nspaceThreadslibrary\nKernel\nspace\n(c) Combined (b) Pure kernel\u2013level (a) Pure user\u2013level\nUser-level thread Kernel-level thread Process\nM04_STAL4290_09_GE_C04.indd   184 5/2/17   4:38 PM\nFigure 4.6  Examples of the R elationships between User-Level Thread States and Process StatesReady Running\nBlockedThread 1\nReady Running\nBlockedThread 2\nReady RunningProcess B(a)\nReady Running\nBlockedThread 1\nReady Running\nBlockedThread 2\nReady RunningProcess B(b)\nReady Running\nBlockedBlocked\nThread 1\nReady Running\nBlockedThread 2\nReady Running\nBlockedProcess B(c)\nReady Running\nBlockedThread 1\nReady Running\nBlockedBlocked\nThread 2\nReady Running\nBlockedProcess B(d)\n185\nM04_STAL4290_09_GE_C04.indd   185 5/2/17   4:38 PM\n186  CHAPTER  4 / T HREADS\nswitches to another process. Meanwhile, according to the data structure main-\ntained by the threads library, thread 2 of process B is still in the Running state. It is important to note that thread 2 is not actually running in the sense of being executed on a processor; but it is perceived as being in the Running state by the threads library. The corresponding state diagrams are shown in Figure 4.6b.\n2.\n A clock interrupt passes contr\nol to the kernel, and the kernel determines \nthat the currently running process (B) has exhausted its time slice. The kernel places process B in the Ready state and switches to another process. Meanwhile, according to the data structure maintained by the threads library, thread 2 of process B is still in the Running state. The corresponding state diagrams are shown in Figure 4.6c.\n3.\n T\nhread 2 has reached a point where it needs some action performed by thread \n1 of process B. Thread 2 enters a Blocked state and thread 1 transitions from Ready to Running. The process itself remains in the Running state. The cor -\nresponding state diagrams are shown in Figure 4.6d.\nNote that each of the three preceding items suggests an alternative event start-\ning from diagram (a) of Figure 4.6. So each of the three other diagrams (b, c, d) shows \na transition from the situation in (a). In cases 1 and 2 (Figures 4.6b and 4.6c), when the \nkernel switches control back to process B, execution resumes in thread 2. Also note that a process can be interrupted, either by exhausting its time slice or by being pre-empted by a higher-priority process, while it is executing code in the threads library. Thus, a process may be in the midst of a thread switch from one thread to another when interrupted. When that process is resumed, execution continues within the threads library, which completes the thread switch and transfers control to another thread within that process.\nThere are a number of advantages to the use of ULTs instead of KLTs, includ-\ning the following:\n1.\n T\nhread switching does not require kernel-mode privileges because all of the \nthread management data structures are within the user address space of a single process. Therefore, the process does not switch to the kernel mode to do thread management. This saves the overhead of two mode switches (user to kernel; kernel back to user).\n2.\n Scheduling can be application specific\n. One application may benefit most from \na simple round-robin scheduling algorithm, while another might benefit from a priority-based scheduling algorithm. The scheduling algorithm can be tailored to the application without disturbing the underlying OS scheduler.\n3.\n UL\nTs can run on any OS. No changes are required to the underlying kernel to \nsupport ULTs. The threads library is a set of application-level functions shared by all applications.\nThere are two distinct disadvantages of ULTs compared to KLTs:\n1.\n In a typical OS\n, many system calls are blocking. As a result, when a ULT exe-\ncutes a system call, not only is that thread blocked, but all of the threads within \nthe process are blocked as well.\nM04_STAL4290_09_GE_C04.indd   186 5/2/17   4:38 PM\n4.2 / TYPES OF THREADS   187\n2. In a pure ULT strategy, a multithreaded application cannot take advantage of \nmultipr\nocessing. A kernel assigns one process to only one processor at a time. \nTherefore, only a single thread within a process can execute at a time. In effect, we have application-level multiprogramming within a single process. While this multiprogramming can result in a significant speedup of the application, there are applications that would benefit from the ability to execute portions of code simultaneously.\nThere are ways to work around these two problems. For example, both prob-\nlems can be overcome by writing an application as multiple processes rather than \nmultiple threads. But this approach eliminates the main advantage of threads: Each switch becomes a process switch rather than a thread switch, resulting in much greater overhead.\nAnother way to overcome the problem of blocking threads is to use a technique \nreferred to as jacketing. The purpose of jacketing is to convert a blocking system call into a nonblocking system call. For example, instead of directly calling a system I/O routine, a thread calls an application-level I/O jacket routine. Within this jacket routine is code that checks to determine if the I/O device is busy. If it is, the thread enters the Blocked state and passes control (through the threads library) to another thread. When this thread is later given control again, the jacket routine checks the I/O device again.\nKerne L-Leve L Thread S In a pure KLT facility, all of the work of thread \nmanagement is done by the kernel. There is no thread management code in the application level, simply an application programming interface (API) to the kernel thread facility. Windows is an example of this approach.\nFigure 4.5b depicts the pure KLT approach. The kernel maintains context infor -\nmation for the process as a whole and for individual threads within the process. Scheduling by the kernel is done on a thread basis. This approach overcomes the two principal drawbacks of the ULT approach. First, the kernel can simultaneously schedule multiple threads from the same process on multiple processors. Second, if one thread in a process is blocked, the kernel can schedule another thread of the same process. Another advantage of the KLT approach is that kernel routines themselves can be multithreaded.\nThe principal disadvantage of the KLT approach compared to the ULT \napproach is that the transfer of control from one thread to another within the same process requires a mode switch to the kernel. To illustrate the differences, \nT\nable\u00a0 4.1 shows the results of measurements taken on a uniprocessor VAX com-\nputer running a UNIX-like OS. The two benchmarks are as follows: Null Fork, the time to create, schedule, execute, and complete a process/thread that invokes \nOperation User-Level Threads Kernel-Level Threads Processes\nNull Fork 34 948 11,300\nSignal Wait 37 441  1,840Table 4.1  Thr ead and Process Operation Latencies (ms)\nM04_STAL4290_09_GE_C04.indd   187 5/2/17   4:38 PM\n188  CHAPTER  4 / T HREADS\nthe\u00a0null procedure\u00a0(i.e., the overhead of forking a process/thread); and Signal-Wait, \nthe time for a process/thread to signal a waiting process/thread and then wait on a condition (i.e., the overhead of synchronizing two processes/threads together). We see there is an order of magnitude or more of difference between ULTs and KLTs, and similarly between KLTs and processes.\nThus, on the face of it, while there is a significant speedup by using KLT mul-\ntithreading compared to single-threaded processes, there is an additional significant speedup by using ULTs. However, whether or not the additional speedup is realized depends on the nature of the applications involved. If most of the thread switches in an application require kernel-mode access, then a ULT-based scheme may not perform much better than a KLT-based scheme.\ncombined  approacheS  Some operating systems provide a combined ULT/KLT \nfacility (see Figure 4.5c). In a combined system, thread creation is done completely in user space, as is the bulk of the scheduling and synchronization of threads within an application. The multiple ULTs from a single application are mapped onto some (smaller or equal) number of KLTs. The programmer may adjust the number of KLTs for a particular application and processor to achieve the best overall results.\nIn a combined approach, multiple threads within the same application can run \nin parallel on multiple processors, and a blocking system call need not block the entire process. If properly designed, this approach should combine the advantages of the pure ULT and KLT approaches while minimizing the disadvantages.\nSolaris is a good example of an OS using this combined approach. The current \nSolaris version limits the ULT/KLT relationship to be one-to-one.\nOther Arrangements\nAs we have said, the concepts of resource allocation and dispatching unit have tra-ditionally been embodied in the single concept of the process\u2014that is, as a 1 : 1 relationship between threads and processes. Recently, there has been much interest in providing for multiple threads within a single process, which is a many-to-one relationship. However, as \nTable 4.2 shows, the other two combinations have also been \ninvestigated, namely, a many-to-many relationship and a one-to-many relationship.\nThreads: Processes Description Example Systems\n1:1 Each thread of execution is a unique process \nwith its own address space and resources.Traditional UNIX implementations\nM:1 A process defines an address space and dynamic resource ownership. Multiple threads may be created and executed within that process.Windows NT, Solaris, Linux, OS/2, OS/390, MACH\n1:M A thread may migrate from one process envi-ronment to another. This allows a thread to be easily moved among distinct systems.Ra (Clouds), Emerald\nM:N It combines attributes of M:1 and 1:M cases. TRIXTable 4.2  Relationship between Threads and Processes\nM04_STAL4290_09_GE_C04.indd   188 5/2/17   4:38 PM\n4.2 / TYPES OF THREADS   189\nmany-To-many reLaTionShip The idea of having a many-to-many relationship \nbetween threads and processes has been explored in the experimental operating \nsystem TRIX [PAZZ92, WARD80]. In TRIX, there are the concepts of domain and thread. A domain is a static entity, consisting of an address space and \u201cports\u201d through which messages may be sent and received. A thread is a single execution path, with an execution stack, processor state, and scheduling information.\nAs with the multithreading approaches discussed so far, multiple threads may \nexecute in a single domain, providing the efficiency gains discussed earlier. However, it is also possible for a single-user activity, or application, to be performed in multiple domains. In this case, a thread exists that can move from one domain to another.\nThe use of a single thread in multiple domains seems primarily motivated by \na desire to provide structuring tools for the programmer. For example, consider a program that makes use of an I/O subprogram. In a multiprogramming environment that allows user-spawned processes, the main program could generate a new process to handle I/O, then continue to execute. However, if the future progress of the main program depends on the outcome of the I/O operation, then the main program will have to wait for the other I/O program to finish. There are several ways to implement this application:\n1.\n T\nhe entire program can be implemented as a single process. This is a reason-\nable and straightforward solution. There are drawbacks related to memory management. The process as a whole may require considerable main memory to execute efficiently, whereas the I/O subprogram requires a relatively small address space to buffer I/O and to handle the relatively small amount of pro-gram code. Because the I/O program executes in the address", "doc_id": "e7033e90-60f5-4941-a7f0-1fb1dea05e60", "embedding": null, "doc_hash": "0ad37ef7af7125ee3ef3adc8a7e6a318f0e6099b2dbb9e03bd42de4ce093df6f", "extra_info": null, "node_info": {"start": 15895, "end": 31566}, "relationships": {"1": "46f5ffc1-aec9-49dc-8b66-426492df7e64", "2": "dc6dfdfb-e39e-44b6-ac60-fc0cc21cf377", "3": "71488cdf-da26-4d3e-b2e4-5d77797ec993"}, "__type__": "1"}, "71488cdf-da26-4d3e-b2e4-5d77797ec993": {"text": "small amount of pro-gram code. Because the I/O program executes in the address space of the larger program, either the entire process must remain in main memory during the I/O operation, or the I/O operation is subject to swapping. This memory manage-ment effect would also exist if the main program and the I/O subprogram were implemented as two threads in the same address space.\n2.\n T\nhe main program and I/O subprogram can be implemented as two separate \nprocesses. This incurs the overhead of creating the subordinate process. If the I/O activity is frequent, one must either leave the subordinate process alive, which consumes management resources, or frequently create and destroy the subprogram, which is inefficient.\n3.\n T\nreat the main program and the I/O subprogram as a single activity that is to \nbe implemented as a single thread. However, one address space (domain) could be created for the main program and one for the I/O subprogram. Thus, the thread can be moved between the two address spaces as execution proceeds. The OS can manage the two address spaces independently, and no process creation overhead is incurred. Furthermore, the address space used by the I/O subprogram could also be shared by other simple I/O programs.\nThe experiences of the TRIX developers indicate that the third option has \nmerit, and may be the most effective solution for some applications.\none-To-many reLaTion Ship In the field of distributed operating systems \n(designed to control distributed computer systems), there has been interest in the \nM04_STAL4290_09_GE_C04.indd   189 5/2/17   4:38 PM\n190  CHAPTER  4 / T HREADS\nconcept of a thread as primarily an entity that can move among address spaces.5 \nA notable example of this research is the Clouds operating system, and especially \nits kernel, known as Ra [DASG92]. Another example is the Emerald system [STEE95].\nA thread in Clouds is a unit of activity from the user\u2019s perspective. A process \nis a virtual address space with an associated process control block. Upon creation, a thread starts executing in a process by invoking an entry point to a program in that process. Threads may move from one address space to another, and actually span computer boundaries (i.e., move from one computer to another). As a thread moves, it must carry with it certain information, such as the controlling terminal, global parameters, and scheduling guidance (e.g., priority).\nThe Clouds approach provides an effective way of insulating both users and \nprogrammers from the details of the distributed environment. A user\u2019s activity may be represented as a single thread, and the movement of that thread among computers may be dictated by the OS for a variety of system-related reasons, such as the need to access a remote resource, and load balancing.\n 4.3 MULTICORE AND MULTITHREADING\nThe use of a multicore system to support a single application with multiple threads (such as might occur on a workstation, a video game console, or a personal computer running a processor-intense application) raises issues of performance and applica-tion design. In this section, we first look at some of the performance implications of a multithreaded application on a multicore system, then describe a specific example of an application designed to exploit multicore capabilities.\nPerformance of Software on Multicore\nThe potential performance benefits of a multicore organization depend on the abil-ity to effectively exploit the parallel resources available to the application. Let us focus first on a single application running on a multicore system. Amdahl\u2019s law\u00a0(see \nAppendix E) states that:\nSpeedup =time to execute program on a single processor\ntime to execute program on N  parallel processors=1\n(1-f)+f\nN\nThe law assumes a program in which a fraction (1-f) of the execution time involves \ncode that is inherently serial, and a fraction f that involves code that is infinitely paral-\nlelizable with no scheduling overhead.\nThis law appears to make the prospect of a multicore organization attractive. \nBut as Figure 4.7a shows, even a small amount of serial code has a noticeable impact. If only 10% of the code is inherently serial \n(f=0.9),  running the program on a multi-\ncore system with eight processors yields a performance gain of a factor of only 4.7 . In \n5The movement of processes or threads among address spaces, or thread migration, on different machines \nhas become a hot topic in recent years.\u00a0Chapter 18  will explore this topic.\nM04_STAL4290_09_GE_C04.indd   190 5/2/17   4:38 PM\n4.3 / MULTICORE AND MULTITHREADING   191\naddition, software typically incurs overhead as a result of communication and distri-\nbution of work to multiple processors and cache coherence overhead. This results in a curve where performance peaks and then begins to degrade because of the increased burden of the overhead of using multiple processors. Figure 4.7b, from [MCDO07], is a representative example.\nHowever, software engineers have been addressing this problem, and there \nare numerous applications in which it is possible to effectively exploit a multicore Figure 4.7  P erformance Effect of Multiple CoresRelative speedup\n02468\n2 1\nNumber of processors\n(a) Speedup with 0%, 2%, 5%, and 10% sequential portions3 4 5 60%\n2%\n5%\n10%\n7 8Relative speedup10%5%\n15%\n20%\n00.51.01.52.02.5\n2 1\nNumber of processors\n(b) Speedup with overheads3 4 5 6 7 8\nM04_STAL4290_09_GE_C04.indd   191 5/2/17   4:38 PM\n192  CHAPTER  4 / T HREADS\nsystem.\u00a0[MCDO07] reports on a set of database applications, in which great attention \nwas paid to reducing the serial fraction within hardware architectures, operating sys-tems, middleware, and the database application software. Figure 4.8 shows the result. As this example shows, database management systems and database applications are one area in which multicore systems can be used effectively. Many kinds of servers can also effectively use the parallel multicore organization, because servers typically handle numerous relatively independent transactions in parallel.\nIn addition to general-purpose server software, a number of classes of applica-\ntions benefit directly from the ability to scale throughput with the number of cores. [MCDO06] lists the following examples:\n\u2022\tMultithreaded native applications: Multithreaded applications are charac-terized by having a small number of highly threaded processes. Examples of threaded applications include Lotus Domino or Siebel CRM (Customer \n R\nelationship Manager).\n\u2022\tMultiprocess applications: Multiprocess applications are characterized by the presence of many single-threaded processes. Examples of multiprocess applica-tions include the Oracle database, SAP , and PeopleSoft.\n\u2022\tJava applications: Java applications embrace threading in a fundamental way. Not only does the Java language greatly facilitate multithreaded applications, but the Java Virtual Machine is a multithreaded process that provides sched-uling and memory management for Java applications. Java applications that can benefit directly from multicore resources include application servers such as Oracle\u2019s Java Application Server, BEA\u2019s Weblogic, IBM\u2019s Websphere, and the open-source Tomcat application server. All applications that use a Java 2 Figure 4.8  Sc aling of Database Workloads on Multiprocessor Hardware0016324864\n16 32\nNumber of CPUsScaling\n48 64perfect scalingOracle DSS 4-way join\nTMC data mining\nDB2 DSS scan & aggs\nOracle ad hoc insurance OLTP\nM04_STAL4290_09_GE_C04.indd   192 5/2/17   4:38 PM\n4.3 / MULTICORE AND MULTITHREADING   193\nPlatform, Enterprise Edition (J2EE platform) application server can immedi-\nately benefit from multicore technology.\n\u2022\tMulti-instance applications: Even if an individual application does not scale to take advantage of a large number of threads, it is still possible to gain from mul-ticore architecture by running multiple instances of the application in parallel. If multiple application instances require some degree of isolation, virtualization technology (for the hardware of the operating system) can be used to provide each of them with its own separate and secure environment.\nApplication Example: Valve Game Software\nValve is an entertainment and technology company that has developed a number of popular games, as well as the Source engine, one of the most widely played game engines available. Source is an animation engine used by Valve for its games and licensed for other game developers.\nIn recent years, Valve has reprogrammed the Source engine software to use \nmultithreading to exploit the power of multicore processor chips from Intel and AMD [REIM06]. The revised Source engine code provides more powerful support for Valve games such as Half Life 2.\nFrom Valve\u2019s perspective, threading granularity options are defined as follows \n[HARR06]:\n\u2022\tCoarse threading: Individual modules, called systems, are assigned to individual processors. In the Source engine case, this would mean putting rendering on one processor, AI (artificial intelligence) on another, physics on another, and so on. This is straightforward. In essence, each major module is single-threaded and the principal coordination involves synchronizing all the threads with a timeline thread.\n\u2022\tFine-grained threading: Many similar or identical tasks are spread across mul-tiple processors. For example, a loop that iterates over an array of data can be split up into a number of smaller parallel loops in individual threads that can be scheduled in parallel.\n\u2022\tHybrid threading: This involves the selective use of fine-grained threading for some systems, and single-threaded for other systems.\nValve found that through coarse threading, it could achieve up to twice the \nperformance across two processors compared to executing on a single processor. But \nthis performance gain could only be achieved with contrived cases. For real-world gameplay, the improvement was on the order of a factor of 1.2. Valve also found that effective use of fine-grained threading was difficult. The time-per-work unit can be variable, and managing the timeline of outcomes and consequences involved complex programming.\nValve found that a hybrid threading approach was the most promising and \nwould scale the best, as multicore systems with 8 or 16 processors became available. Valve identified systems that operate very effectively being permanently assigned to a single processor. An example is sound mixing, which has little user interaction, is not constrained by the frame configuration of windows, and works on its own set \nM04_STAL4290_09_GE_C04.indd   193 5/2/17   4:38 PM\n194  CHAPTER  4 / T HREADS\nof data. Other modules, such as scene rendering, can be organized into a number of \nthreads so that the module can execute on a single processor but achieve greater performance as it is spread out over more and more processors.\nFigure 4.9 illustrates the thread structure for the rendering module. In this \nhierarchical structure, higher-level threads spawn lower-level threads as needed. The rendering module relies on a critical part of the Source engine, the world list, which is a database representation of the visual elements in the game\u2019s world. The first task is to determine what are the areas of the world that need to be rendered. The next task is to determine what objects are in the scene as viewed from multiple angles. Then comes the processor-intensive work. The rendering module has to work out the rendering of each object from multiple points of view, such as the player\u2019s view, the view of monitors, and the point of view of reflections in water.\nSome of the key elements of the threading strategy for the rendering module \nare listed in [LEON07] and include the following:\n\u2022\tConstruct scene rendering lists for multiple scenes in parallel (e.g., the world and its reflection in water).\n\u2022\tOverlap graphics simulation.\n\u2022\tCompute character bone transformations for all characters in all scenes in parallel.\n\u2022\tAllow multiple threads to draw in parallel.Figure 4.9  Hybrid Threading for Rendering ModuleRender\nSkybox Main view\nScene list\nFor each object\nParticles\nSim and draw\nBone setup\nDrawCharacter\nEtc.Monitor Etc.\nM04_STAL4290_09_GE_C04.indd   194 5/2/17   4:38 PM\n4.4 / WINDOWS PROCESS AND THREAD MANAGEMENT   195\nThe designers found that simply locking key databases, such as the world list, \nfor a thread was too inefficient. Over 95% of the time, a thread is trying to read from \na data set, and only 5% of the time at most is spent in writing to a data set. Thus, a concurrency mechanism known as the single-writer-multiple-readers model works effectively.\n 4. 4 WINDO WS PROCESS AND THREAD MANAGEMENT\nThis section begins with an overview of the key objects and mechanisms that support application execution in Windows. The remainder of the section looks in more detail at how processes and threads are managed.\nAn application consists of one or more processes. Each process provides the \nresources needed to execute a program. A process has a virtual address space, exe-cutable code, open handles to system objects, a security context, a unique process identifier, environment variables, a priority class, minimum and maximum working set sizes, and at least one thread of execution. Each process is started with a single thread, often called the primary thread, but can create additional threads from any of its threads.\nA thread is the entity within a process that can be scheduled for execution. \nAll threads of a process share its virtual address space and system resources. In addition, each thread maintains exception handlers, a scheduling priority, thread local storage, a unique thread identifier, and a set of structures the system will use to save the thread context until it is scheduled. On a multiprocessor computer, the system can simultaneously execute as many threads as there are processors on the computer.\nA job object allows groups of processes to be managed as a unit. Job objects \nare namable, securable, sharable objects that control attributes of the processes associated with them. Operations performed on the job object affect all processes associated with the job object. Examples include enforcing limits such as working set\u00a0size\u00a0and process priority or terminating all processes associated with a job.\nA thread pool is a collection of worker threads that efficiently execute asyn-\nchronous callbacks on behalf of the application. The thread pool is primarily used to reduce the number of application threads and provide management of the worker threads.\nA fiber is a unit of execution that must be manually scheduled by the applica-\ntion. Fibers run in the context of the threads that schedule them. Each thread can schedule multiple fibers. In general, fibers do not provide advantages over a well-designed multithreaded application. However, using fibers can make it easier to port applications that were designed to schedule their own threads. From a system stand-point, a fiber assumes the identity of the thread that runs it. For example if a fiber accesses thread local storage, it is accessing the thread local storage of the thread that is running it. In addition, if a fiber calls the ExitThread function, the thread that is running it exits. However, a fiber does not have all the same state information associ-ated with it as that associated with a thread. The only state information maintained for a fiber is its stack, a subset of its registers, and the fiber data provided during fiber creation. The saved registers are the set of registers typically preserved across \nM04_STAL4290_09_GE_C04.indd   195 5/2/17   4:38 PM\n196  CHAPTER  4 / T HREADS\na function call. Fibers are not preemptively scheduled. A thread schedules a fiber by \nswitching to it from another fiber. The system still schedules threads to run. When a thread that is running fibers is preempted, its currently running fiber is preempted but remains", "doc_id": "71488cdf-da26-4d3e-b2e4-5d77797ec993", "embedding": null, "doc_hash": "602875c17d199adeb3ca000c1bb76f05c9c6345f1f55165e62bc4f8b16805ff4", "extra_info": null, "node_info": {"start": 31581, "end": 47490}, "relationships": {"1": "46f5ffc1-aec9-49dc-8b66-426492df7e64", "2": "e7033e90-60f5-4941-a7f0-1fb1dea05e60", "3": "c7c63ae5-7725-4b1f-9fcc-62b5991a609b"}, "__type__": "1"}, "c7c63ae5-7725-4b1f-9fcc-62b5991a609b": {"text": "fibers is preempted, its currently running fiber is preempted but remains selected.\nUser-mode scheduling (UMS) is a lightweight mechanism that applications can \nuse to schedule their own threads. An application can switch between UMS threads in user mode without involving the system scheduler, and regain control of the proces-sor if a UMS thread blocks in the kernel. Each UMS thread has its own thread context instead of sharing the thread context of a single thread. The ability to switch between threads in user mode makes UMS more efficient than thread pools for short-duration work items that require few system calls. UMS is useful for applications with high performance requirements that need to efficiently run many threads concurrently on multiprocessor or multicore systems. To take advantage of UMS, an application must implement a scheduler component that manages the application\u2019s UMS threads and determines when they should run.\nManagement of Background Tasks and Application \nLifecycles\nBeginning with Windows 8, and carrying through to Windows 10, developers are \nresponsible for managing the state of their individual applications. Previous versions of Windows always give the user full control of the lifetime of a process. In the classic desktop environment, a user is responsible for closing an application. A dialog box might prompt them to save their work. In the new Metro interface, Windows takes over the process lifecycle of an application. Although a limited number of applica-tions can run alongside the main app in the Metro UI using the SnapView function-ality, only one Store application can run at one time. This is a direct consequence of the new design. Windows Live Tiles give the appearance of applications constantly running on the system. In reality, they receive push notifications and do not use sys-tem resources to display the dynamic content offered.\nThe foreground application in the Metro interface has access to all of the \nprocessor, network, and disk resources available to the user. All other apps are suspended and have no access to these resources. When an app enters a suspended mode, an event should be triggered to store the state of the user\u2019s information. This is the responsibility of the application developer. For a variety of reasons, whether it needs resources or because an application timed out, Windows may terminate a background app. This is a significant departure from the Windows operating systems that precede it. The app needs to retain any data the user entered, settings they changed, and so on. That means you need to save your app\u2019s state when it\u2019s suspended, in case Windows terminates it, so you can restore its state later. When the app returns to the foreground, another event is triggered to obtain the user state from memory. No event fires to indicate termination of a background app. Rather, the application data will remain resident on the system, as though it is suspended, until the app is launched again. Users expect to find the app as they left it, whether it was suspended or terminated by Windows, or closed by the user. Application developers can use code to determine whether it should restore a saved state.\nM04_STAL4290_09_GE_C04.indd   196 5/2/17   4:38 PM\n4.4 / WINDOWS PROCESS AND THREAD MANAGEMENT   197\nSome applications, such as news feeds, may look at the date stamp associated \nwith the previous execution of the app and elect to discard the data in favor of newly \nobtained information. This is a determination made by the developer, not by the oper -\nating system. If the user closes an app, unsaved data is not saved. With foreground tasks occupying all of the system resources, starvation of background apps is a reality in Windows. This makes the application development relating to the state changes critical to the success of a Windows app.\nTo process the needs of background tasks, a background task API is built to \nallow apps to perform small tasks while not in the foreground. In this restricted envi-ronment, apps may receive push notifications from a server or a user may receive a phone call. Push notifications are template XML strings. They are managed through a cloud service known as the Windows Notification Service (WNS). The service will occasionally push updates to the user\u2019s background apps. The API will queue those requests and process them when it receives enough processor resources. Background tasks are severely limited in the usage of processor, receiving only one proces-sor second per processor hour. This ensures that critical tasks receive guaranteed application resource quotas. It does not, however, guarantee a background app will ever run.\nThe Windows Process\nImportant characteristics of Windows processes are the following:\n\u2022\tWindows processes are implemented as objects.\n\u2022\tA process can be created as a new process or as a copy of an existing process.\n\u2022\tAn executable process may contain one or more threads.\n\u2022\tBoth process and thread objects have built-in synchronization capabilities.\nFigure 4.10, based on one in [RUSS11], illustrates the way in which a process \nrelates to the resources it controls or uses. Each process is assigned a security access \ntoken, called the primary token of the process. When a user first logs on, Windows creates an access token that includes the security ID for the user. Every process that is created by or runs on behalf of this user has a copy of this access token. Windows uses the token to validate the user\u2019s ability to access secured objects, or to perform restricted functions on the system and on secured objects. The access token controls whether the process can change its own attributes. In this case, the process does not have a handle opened to its access token. If the process attempts to open such a handle, the security system determines whether this is permitted, and therefore whether the process may change its own attributes.\nAlso related to the process is a series of blocks that define the virtual address \nspace currently assigned to this process. The process cannot directly modify these structures, but must rely on the virtual memory manager, which provides a memory-allocation service for the process.\nFinally, the process includes an object table, with handles to other objects \nknown to this process. Figure 4. 10 shows a single thread. In addition, the process \nhas access to a file object and to a section object that defines a section of shared memory.\nM04_STAL4290_09_GE_C04.indd   197 5/2/17   4:38 PM\n198  CHAPTER  4 / T HREADS\nProcess and Thread Objects\nThe object-oriented structure of Windows facilitates the development of a general-\npurpose process facility. Windows makes use of two types of process-related objects: processes and threads. A process is an entity corresponding to a user job or applica-tion that owns resources, such as memory and open files. A thread is a dispatchable unit of work that executes sequentially and is interruptible, so the processor can turn to another thread.\nEach Windows process is represented by an object. Each process object \nincludes a number of attributes and encapsulates a number of actions, or services, that it may perform. A process will perform a service when called upon through a set of published interface methods. When Windows creates a new process, it uses the object class, or type, defined for the Windows process as a template to gener -\nate a new object instance. At the time of creation, attribute values are assigned. \nT\nable 4.3 gives a brief definition of each of the object attributes for a process \nobject.\nA Windows process must contain at least one thread to execute. That thread \nmay then create other threads. In a multiprocessor system, multiple threads from the same process may execute in parallel. Table \n4.4 defines the thread object attributes. \nNote some of the attributes of a thread resemble those of a process. In those cases, the thread attribute value is derived from the process attribute value. For example, the thread processor affinity is the set of processors in a multiprocessor system that \nmay execute this thread; this set is equal to or a subset of the process processor affinity.Figure 4.10  A W indows Process and Its ResourcesProcess\nobjectAccess\ntoken\nVirtual address descriptors\nThread x \nFile y \nSection z Handle1\nHandle2Handle3Available\nobjects Handle table\nM04_STAL4290_09_GE_C04.indd   198 5/2/17   4:38 PM\n4.4 / WINDOWS PROCESS AND THREAD MANAGEMENT   199\nNote one of the attributes of a thread object is context, which contains the \nvalues of the processor registers when the thread last ran. This information enables \nthreads to be suspended and resumed. Furthermore, it is possible to alter the behavior of a thread by altering its context while it is suspended.Process ID A unique value that identifies the process to the operating system.\nSecurity descriptor Describes who created an object, who can gain access to or use the object, and \nwho is denied access to the object.\nBase priority A baseline execution priority for the process\u2019s threads.\nDefault processor affinity The default set of processors on which the process\u2019s threads can run.\nQuota limits The maximum amount of paged and nonpaged system memory, paging file space, and processor time a user\u2019s processes can use.\nExecution time The total amount of time all threads in the process have executed.\nI/O counters Variables that record the number and type of I/O operations that the process\u2019s threads have performed.\nVM operation counters Variables that record the number and types of virtual memory operations that the process\u2019s threads have performed.\nException/debugging ports Interprocess communication channels to which the process manager sends a message when one of the process\u2019s threads causes an exception. Normally, these are connected to environment subsystem and debugger processes, respectively.\nExit status The reason for a process\u2019s termination.Table 4.3  W indows Process Object Attributes\nThread ID A unique value that identifies a thread when it calls a server.\nThread context The set of register values and other volatile data that defines the execution state of a thread.\nDynamic priority The thread\u2019s execution priority at any given moment.\nBase priority The lower limit of the thread\u2019s dynamic priority.\nThread processor affinity The set of processors on which the thread can run, which is a subset or all of the processor affinity of the thread\u2019s process.\nThread execution time The cumulative amount of time a thread has executed in user mode and in \n k\nernel mode.\nAlert status A flag that indicates whether a waiting thread may execute an asynchronous procedure call.\nSuspension count The number of times the thread\u2019s execution has been suspended without being resumed.\nImpersonation token A temporary access token allowing a thread to perform operations on behalf of another process (used by subsystems).\nTermination port An interprocess communication channel to which the process manager sends a message when the thread terminates (used by subsystems).\nThread exit status The reason for a thread\u2019s termination.Table 4.4  W indows Thread Object Attributes\nM04_STAL4290_09_GE_C04.indd   199 5/2/17   4:38 PM\n200  CHAPTER  4 / T HREADS\nMultithreading\nWindows supports concurrency among processes because threads in different \n p\nrocesses may execute concurrently (appear to run at the same time). Moreover, \n multiple thr\neads within the same process may be allocated to separate processors \nand execute simultaneously (actually run at the same time). A multithreaded pro-\ncess achieves concurrency without the overhead of using multiple processes. Threads within the same process can exchange information through their common address space and have access to the shared resources of the process. Threads in different processes can exchange information through shared memory that has been set up between the two processes.\nAn object-oriented multithreaded process is an efficient means of implement-\ning a server application. For example, one server process can service a number of clients concurrently.\nThread States\nAn existing Windows thread is in one of six states (see Figure 4.11):\n1.\n R\neady: A ready thread may be scheduled for execution. The Kernel dispatcher \nkeeps track of all ready threads and schedules them in priority order.\n2.\n S\ntandby: A standby thread has been selected to run next on a particular proces -\nsor. The thread waits in this state until that processor is made available. If the standby thread\u2019s priority is high enough, the running thread on that processor may be preempted in favor of the standby thread. Otherwise, the standby thread waits until the running thread blocks or exhausts its time slice.\nFigure 4.11  W indows Thread StatesRunnable\nNot runnablePick to\nrunSwitch\nPreempted\nBlock/\nsuspendUnblock/resume\nResource availableResource\navailable\nUnblock\nResource not availableTerminateStandby\nReady Running\nTransition Waiting Terminated\nM04_STAL4290_09_GE_C04.indd   200 5/2/17   4:38 PM\n4.4 / WINDOWS PROCESS AND THREAD MANAGEMENT   201\n3. R unning: Once the Kernel dispatcher performs a thread switch, the standby \nthread enters the Running state and begins execution and continues execution \nuntil it is preempted by a higher-priority thread, exhausts its time slice, blocks, or terminates. In the first two cases, it goes back to the Ready state.\n4.\n W\naiting: A thread enters the Waiting state when (1) it is blocked on an event \n(e.g., I/O), (2) it voluntarily waits for synchronization purposes, or (3) an envi-ronment subsystem directs the thread to suspend itself. When the waiting con-dition is satisfied, the thread moves to the Ready state if all of its resources are available.\n5.\n T\nransition: A thread enters this state after waiting if it is ready to run, but the \nresources are not available. For example, the thread\u2019s stack may be paged out of memory. When the resources are available, the thread goes to the Ready state.\n6.\n T\nerminated: A thread can be terminated by itself, by another thread, or when \nits parent process terminates. Once housekeeping chores are completed, the thread is removed from the system, or it may be retained by the Executive\n6 for \nfuture reinitialization.\nSupport for OS Subsystems\nThe general-purpose process and thread facility must support the particular process and thread structures of the various OS environments. It is the responsibility of each OS subsystem to exploit the Windows process and thread features to emulate the process and thread facilities of its corresponding OS. This area of process/thread management is complicated, and we give only a brief overview here.\nProcess creation begins with a request for a new process from an application. \nThe application issues a create-process request to the corresponding protected sub-system, which passes the request to the Executive. The Executive creates a process object and returns a handle for that object to the subsystem. When Windows creates a process, it does not automatically create a thread. In the case of Win32, a new pro-cess must always be created with an initial thread. Therefore, the Win32 subsystem calls the Windows process manager again to create a thread for the new process, receiving a thread handle back from Windows. The appropriate thread and process information are then returned to the application. In the case of POSIX, threads are not supported. Therefore, the POSIX subsystem obtains a thread for the new process from Windows so that the process may be activated but returns only process informa-tion to the application. The fact that the POSIX process is implemented using both a process and a thread from the Windows Executive is not visible to the application.\nWhen a new process is created by the Executive, the new process inherits many of \nits attributes from the creating process. However, in the Win32 environment, this pro-cess creation is done indirectly. An application client process issues its process creation request to the Win32 subsystem; then the subsystem in turn issues a process request to the Windows executive. Because the desired effect is that the new process inherits characteristics of the client process and not of the server process, Windows enables the \n6The Windows Executive\u00a0is described in Chapter 2. It  contains the base operating system services, such as \nmemory management, process and thread management, security, I/O, and interprocess communication.\nM04_STAL4290_09_GE_C04.indd   201 5/2/17   4:38 PM\n202  CHAPTER  4 / T HREADS\nsubsystem to specify the parent of the new process. The new process then inherits the \nparent\u2019s access token, quota limits, base priority, and default processor affinity.\n 4. 5 SOLARIS THREAD AND SMP MAN AGEMENT\nSolaris implements multilevel thread support designed to provide considerable flex -\nibility in exploiting processor resources.\nMultithreaded Architecture\nSolaris makes", "doc_id": "c7c63ae5-7725-4b1f-9fcc-62b5991a609b", "embedding": null, "doc_hash": "f973d12a1d762ed2b79bd2fb104c94709b5d98818bce4e28e275a926e2d6b00c", "extra_info": null, "node_info": {"start": 47494, "end": 64428}, "relationships": {"1": "46f5ffc1-aec9-49dc-8b66-426492df7e64", "2": "71488cdf-da26-4d3e-b2e4-5d77797ec993", "3": "8ce0d0ef-4e9b-4d84-a745-9abd388b991f"}, "__type__": "1"}, "8ce0d0ef-4e9b-4d84-a745-9abd388b991f": {"text": "in exploiting processor resources.\nMultithreaded Architecture\nSolaris makes use of four separate thread-related concepts:\n1.\n Pr\nocess: This is the normal UNIX process and includes the user\u2019s address space, \nstack, and process control block.\n2.\n User\n-level threads: Implemented through a threads library in the address space \nof a process, these are invisible to the OS. A user-level thread (ULT)7 is a user-\ncreated unit of execution within a process.\n3.\n Lightw\neight processes: A lightweight process (L WP) can be viewed as a map-\nping between ULTs and kernel threads. Each L WP supports ULT and maps to one kernel thread. L WPs are scheduled by the kernel independently, and may execute in parallel on multiprocessors.\n4.\n K\nernel threads: These are the fundamental entities that can be scheduled and \ndispatched to run on one of the system processors.\nFigure 4.12 illustrates the relationship among these four entities. Note there is \nalways exactly one kernel thread for each L WP . An L WP is visible within a process to \nthe application. Thus, L WP data structures exist within their respective process address space. At the same time, each L WP is bound to a single dispatchable kernel thread, and the data structure for that kernel thread is maintained within the kernel\u2019s address space.\nA process may consist of a single ULT bound to a single L WP . In this case, there \nis a single thread of execution, corresponding to a traditional UNIX process. When concurrency is not required within a single process, an application uses this process structure. If an application requires concurrency, its process contains multiple threads, each bound to a single L WP , which in turn are each bound to a single kernel thread.\nIn addition, there are kernel threads that are not associated with L WPs. The \nkernel creates, runs, and destroys these kernel threads to execute specific system functions. The use of kernel threads rather than kernel processes to implement system functions reduces the overhead of switching within the kernel (from a process switch to a thread switch).\nMotivation\nThe three-level thread structure (ULT, L WP , kernel thread) in Solaris is intended to facilitate thread management by the OS and to provide a clean interface to appli-cations. The ULT interface can be a standard thread library. A defined ULT maps onto a L WP , which is managed by the OS and which has defined states of execution, \n7Again, the acronym ULT is unique to this book and is not found in the Solaris literature.\nM04_STAL4290_09_GE_C04.indd   202 5/2/17   4:38 PM\n4.5 / SOLARIS THREAD AND SMP MANAGEMENT   203\ndefined subsequently. An L WP is bound to a kernel thread with a one-to-one cor -\nrespondence in execution states. Thus, concurrency and execution are managed at \nthe level of the kernel thread.\nIn addition, an application has access to hardware through an application pro-\ngramming interface consisting of system calls. The API allows the user to invoke ker -\nnel services to perform privileged tasks on behalf of the calling process, such as read or write a file, issue a control command to a device, create a new process or thread, allocate memory for the process to use, and so on.\nProcess Structure\nFigure 4.13 compares, in general terms, the process structure of a traditional \nUNIX system with that of Solaris. On a typical UNIX implementation, the process structure includes:\n\u2022\tProcess ID.\n\u2022\tUser IDs.\n\u2022\tSignal dispatch table, which the kernel uses to decide what to do when sending a signal to a process.\n\u2022\tFile descriptors, which describe the state of files in use by this process.\n\u2022\tMemory map, which defines the address space for this process.\n\u2022\tProcessor state structure, which includes the kernel stack for this process.\nSolaris retains this basic structure but replaces the processor state block with a list of structures containing one data block for each L WP .\nThe L WP data structure includes the following elements:\n\u2022\tAn L WP identifier\n\u2022\tThe priority of this L WP and hence the kernel thread that supports itFigure 4.12  Pr ocesses and Threads in Solaris [MCDO07]HardwareKernelSystem callssyscall() syscall()Process\nKernel\nthreadKernel\nthreadLightweight\nprocess (LWP)Lightweight\nprocess (LWP)User\nthreadUser\nthread\nM04_STAL4290_09_GE_C04.indd   203 5/2/17   4:38 PM\n204  CHAPTER  4 / T HREADS\n\u2022\tA signal mask that tells the kernel which signals will be accepted\n\u2022\tSaved values of user-level registers (when the L WP is not running)\n\u2022\tThe kernel stack for this L WP , which includes system call arguments, results, and \nerror codes for each call level\n\u2022\tResource usage and profiling data\n\u2022\tPointer to the corresponding kernel thread\n\u2022\tPointer to the process structure\nThread Execution\nFigure 4.14 shows a simplified view of both thread execution states. These states reflect the execution status of both a kernel thread and the L WP bound to it. As mentioned, some kernel threads are not associated with an L WP; the same execution diagram applies. The states are as follows:\n\u2022\tRUN: The thread is runnable; that is, the thread is ready to execute.\n\u2022\tONPROC: The thread is executing on a processor.Figure 4.13  Pr ocess Structure in Traditional UNIX and Solaris [LEWI96]Process IDUNIX process structure\nUser IDs\nSignal dispatch table\nFile descriptorsMemory map\nPriority\nSignal mask\nRegisters\nSTACK\nLWP IDProcessor stateProcess IDSolaris process structure\nUser IDs\nSignal dispatch table\nFile descriptors\nLWP 1\nLWP IDLWP 2Memory map\nPriority\nSignal mask\nRegisters\nSTACKPriority\nSignal mask\nRegisters\nSTACK\nM04_STAL4290_09_GE_C04.indd   204 5/2/17   4:38 PM\n4.5 / SOLARIS THREAD AND SMP MANAGEMENT   205\n\u2022\tSLEEP: The thread is blocked.\n\u2022\tSTOP: The thread is stopped.\n\u2022\tZOMBIE: The thread has terminated.\n\u2022\tFREE: Thread resources have been released and the thread is awaiting removal \nfrom the OS thread data structure.\nA thread moves from ONPROC to RUN if it is preempted by a higher-priority \nthread or because of time slicing. A thread moves from ONPROC to SLEEP if it \nis blocked and must await an event to return the RUN state. Blocking occurs if the thread invokes a system call and must wait for the system service to be performed. A thread enters the STOP state if its process is stopped; this might be done for debug-ging purposes.\nInterrupts as Threads\nMost operating systems contain two fundamental forms of concurrent activity: pro-cesses and interrupts. Processes (or threads) cooperate with each other and manage the use of shared data structures by means of a variety of primitives that enforce mutual exclusion (only one process at a time can execute certain code or access certain data) and that synchronize their execution. Interrupts are synchronized by preventing their handling for a period of time. Solaris unifies these two concepts into a single model, namely kernel threads, and the mechanisms for scheduling and executing kernel threads. To do this, interrupts are converted to kernel threads.Figure 4.14  Solaris Thr ead StatesIDL\nthread_create() intr()\nswtch()\nsyscall()\nwakeup()\nprun() pstop() exit() reap()preempt()RUNPINNED\nSLEEP\nSTOP ZOMBIE FREE\nONPROC\nM04_STAL4290_09_GE_C04.indd   205 5/2/17   4:38 PM\n206  CHAPTER  4 / T HREADS\nThe motivation for converting interrupts to threads is to reduce overhead. \nInterrupt handlers often manipulate data shared by the rest of the kernel. There-\nfore, while a kernel routine that accesses such data is executing, interrupts must be blocked, even though most interrupts will not affect that data. Typically, the way this is done is for the routine to set the interrupt priority level higher to block interrupts, then lower the priority level after access is completed. These operations take time. The problem is magnified on a multiprocessor system. The kernel must protect more objects and may need to block interrupts on all processors.\nThe solution in Solaris can be summarized as follows:\n1.\n Solaris employs a set of k\nernel threads to handle interrupts. As with any kernel \nthread, an interrupt thread has its own identifier, priority, context, and stack.\n2.\n T\nhe kernel controls access to data structures and synchronizes among inter -\nrupt threads using mutual exclusion primitives, of the type to be discussed in Chapter 5. That is, the normal synchronization techniques for threads are used in handling interrupts.\n3.\n Interrupt thr\neads are assigned higher priorities than all other types of kernel \nthreads.\nWhen an interrupt occurs, it is delivered to a particular processor and the thread \nthat was executing on that processor is pinned. A pinned thread cannot move to \nanother processor and its context is preserved; it is simply suspended until the inter -\nrupt is processed. The processor then begins executing an interrupt thread. There is a pool of deactivated interrupt threads available, so a new thread creation is not required. The interrupt thread then executes to handle the interrupt. If the handler routine needs access to a data structure that is currently locked in some fashion for use by another executing thread, the interrupt thread must wait for access to that data structure. An interrupt thread can only be preempted by another interrupt thread of higher priority.\nExperience with Solaris interrupt threads indicates that this approach provides \nsuperior performance to the traditional interrupt-handling strategy [KLEI95].\n 4. 6 LINUX PR OCESS AND THREAD MANAGEMENT\nLinux Tasks\nA process, or task, in Linux is represented by a task_struct data structure. The task_struct data structure contains information in a number of categories:\n\u2022\tState: The execution state of the process (executing, ready, suspended, stopped, zombie). This is described subsequently.\n\u2022\tScheduling information: Information needed by Linux to schedule processes. A\u00a0process can be normal or real time and has a priority. Real-time processes are scheduled before normal processes, and within each category, relative priorities can be used. A reference counter keeps track of the amount of time a process is allowed to execute.\n\u2022\tIdentifiers: Each process has a unique process identifier (PID) and also has user and group identifiers. A group identifier is used to assign resource access privileges to a group of processes.\nM04_STAL4290_09_GE_C04.indd   206 5/2/17   4:38 PM\n4.6 / LINUX PROCESS AND THREAD MANAGEMENT   207\n\u2022\tInterprocess communication:  Linux supports the IPC mechanisms found in \nUNIX SVR4, described later in Chapter 6.\n\u2022\tLinks: Each process includes a link to its parent process, links to its siblings \n(processes with the same parent), and links to all of its children.\n\u2022\tTimes and timers: Includes process creation time and the amount of processor time so far consumed by the process. A process may also have associated one or more interval timers. A process defines an interval timer by means of a system call; as a result, a signal is sent to the process when the timer expires. A timer may be single use or periodic.\n\u2022\tFile system: Includes pointers to any files opened by this process, as well as pointers to the current and the root directories for this process\n\u2022\tAddress space: Defines the virtual address space assigned to this process\n\u2022\tProcessor-specific context: The registers and stack information that constitute the context of this process\nFigure 4.15 shows the execution states of a process. These are as follows:\n\u2022\tRunning: This state value corresponds to two states. A Running process is either \nexecuting, or it is ready to execute.\n\u2022\tInterruptible: This is a blocked state, in which the process is waiting for an event, such as the end of an I/O operation, the availability of a resource, or a signal from another process.\nFigure 4.15  Linux Pr ocess/Thread ModelRunning\nstate\nCreationSchedulingTerminationSignal Signal\nEvent\nSignal\nor\neventStopped\nReady Executing Zombie\nUninterruptible\nInterruptible\nM04_STAL4290_09_GE_C04.indd   207 5/2/17   4:38 PM\n208  CHAPTER  4 / T HREADS\n\u2022\tUninterruptible: This is another blocked state. The difference between this and \nthe Interruptible state is that in an Uninterruptible state, a process is waiting directly on hardware conditions and therefore will not handle any signals.\n\u2022\tStopped: The process has been halted and can only resume by positive action from another process. For example, a process that is being debugged can be put into the Stopped state.\n\u2022\tZombie: The process has been terminated but, for some reason, still must have its task structure in the process table.\nLinux Threads\nTraditional UNIX systems support a single thread of execution per process, while modern UNIX systems typically provide support for multiple kernel-level threads per process. As with traditional UNIX systems, older versions of the Linux kernel offered no support for multithreading. Instead, applications would need to be written with a set of user-level library functions, the most popular of which is known as pthread (POSIX thread) libraries , with all of the threads mapping into a single kernel-\nlevel process.\n8 We have seen that modern versions of UNIX offer kernel-level threads. \nLinux provides a unique solution in that it does not recognize a distinction between threads and processes. Using a mechanism similar to the lightweight processes of Solaris, user-level threads are mapped into kernel-level processes. Multiple user-level threads that constitute a single user-level process are mapped into Linux kernel-level processes that share the same group ID. This enables these processes to share resources such as files and memory, and to avoid the need for a context switch when the scheduler switches among processes in the same group.\nA new process is created in Linux by copying the attributes of the current \nprocess. A new process can be cloned so it shares resources such as files, signal han-dlers, and virtual memory. When the two processes share the same virtual memory, they function as threads within a single process. However, no separate type of data structure is defined for a thread. In place of the usual fork() command, processes are created in Linux using the clone() command. This command includes a set of flags as arguments. The traditional fork() system call is implemented by Linux as a clone() system call with all of the clone flags cleared.\nExamples of clone flags include the following:\n\u2022\tCLONE_NEWPID: Creates new process ID namespace.\n\u2022\tCLONE_PARENT: Caller and new task share the same parent process.\n\u2022\tCLONE_SYSVSEM: Shares System V SEM_UNDO semantics.\n\u2022\tCLONE_THREAD: Inserts this process into the same thread group of the par -\nent. If this flag is true, it implicitly enforces CLONE_PARENT.\n\u2022\tCLONE_VM: Shares the address space (memory descriptor and all page tables).\n8POSIX (Portable Operating Systems based on UNIX) is an IEEE API standard that includes a standard \nfor a thread API. Libraries implementing the POSIX Threads standard are often named Pthreads. Pthreads \nare most commonly used on UNIX-like POSIX systems such as Linux and Solaris, but Microsoft Windows implementations also exist.\nM04_STAL4290_09_GE_C04.indd   208 5/2/17   4:38 PM\n4.6 / LINUX PROCESS AND THREAD MANAGEMENT   209\n\u2022\tCLONE_FS: Shares the same filesystem information (including current work-\ning directory, the root of the filesystem, and the umask).\n\u2022\tCLONE_FILES: Shares the same file descriptor table. Creating a file descrip-tor or closing a file descriptor is propagated to the another process, as well as changing the associated flags of a file descriptor using the", "doc_id": "8ce0d0ef-4e9b-4d84-a745-9abd388b991f", "embedding": null, "doc_hash": "f9290d9e387de4cb4895fd8b3095ab541cdccdbef81f5e851a91f3ddb68ff7ed", "extra_info": null, "node_info": {"start": 64422, "end": 79908}, "relationships": {"1": "46f5ffc1-aec9-49dc-8b66-426492df7e64", "2": "c7c63ae5-7725-4b1f-9fcc-62b5991a609b", "3": "f207099d-ecea-402a-874e-63874d453c3c"}, "__type__": "1"}, "f207099d-ecea-402a-874e-63874d453c3c": {"text": "to the another process, as well as changing the associated flags of a file descriptor using the fcntl() system call.\nWhen the Linux kernel performs a context switch from one process to another, \nit checks whether the address of the page directory of the current process is the same \nas that of the to-be-scheduled process. If they are, then they are sharing the same address space, so a context switch is basically just a jump from one location of code to another location of code.\nAlthough cloned processes that are part of the same process group can share \nthe same memory space, they cannot share the same user stacks. Thus the clone() call creates separate stack spaces for each process.\nLinux Namespaces\nAssociated with each process in Linux are a set of namespaces. A namespace enables a process (or multiple processes that share the same namespace) to have a differ -\nent view of the system than other processes that have other associated namespaces. Namespaces and cgroups (which will be described in the following section) are the basis of Linux lightweight virtualization, which is a feature that provides a process or group of processes with the illusion that they are the only processes on the system. This \n featur\ne is used widely by Linux Containers projects. There are currently six \nnamespaces in Linux: mnt, pid, net, ipc, uts, and user.\nNamespaces are created by the clone() system call, which gets as a param-\neter one of the six namespaces clone flags (CLONE_NEWNS, CLONE_NEWPID, CLONE_NEWNET, CLONE_NEWIPC, CLONE_NEWUTS, and CLONE_NEWUSER). A process can also create a namespace with the unshare() system call with one of these flags; as opposed to clone(), a new process is not created in such a case; only a new namespace is created, which is attached to the calling process.\nmoUnT  nameSpace  A mount namespace provides the process with a specific view \nof the filesystem hierarchy, such that two processes with different mount namespaces see different filesystem hierarchies. All of the file operations that a process employs apply only to the filesystem visible to the process.\nUTS name Space  The UTS (UNIX timesharing) namespace is related to the uname \nLinux system call. The uname call returns the name and information about the current kernel, including nodename, which is the system name within some implementation-defined network; and domainname, which is the NIS domain name. NIS (Network Information Service) is a standard scheme used on all major UNIX and UNIX-like systems. It allows a group of machines within an NIS domain to share a common set of configuration files. This permits a system administrator to set up NIS client systems with only minimal configuration data and add, remove, or modify configuration data from a single location. With the UTS namespace, initialization and configuration parameters can vary for different processes on the same system.\nM04_STAL4290_09_GE_C04.indd   209 5/2/17   4:38 PM\n210  CHAPTER  4 / T HREADS\nipc nameSpace  An IPC namespace isolates certain interprocess communication \n(IPC) resources, such as semaphores, POSIX message queues, and more. Thus, \nconcurrency mechanisms can be employed by the programmer that enable IPC among processes that share the same IPC namespace.\npid name Space  PID namespaces isolate the process ID space, so processes in \ndifferent PID namespaces can have the same PID. This feature is used for Checkpoint/Restore In Userspace (CRIU), a Linux software tool. Using this tool, you can freeze a running application (or part of it) and checkpoint it to a hard drive as a collection of files. You can then use the files to restore and run the application from the freeze point on that machine or on a different host. A distinctive feature of the CRIU project is that it is mainly implemented in user space, after attempts to implement it mainly in kernel failed.\nneTwor K name Space  Network namespaces provide isolation of the system \nresources associated with networking. Thus, each network namespace has its own network devices, IP addresses, IP routing tables, port numbers, and so on. These namespaces virtualize all access to network resources. This allows each process or a group of processes that belong to this network namespace to have the network access it needs (but no more). At any given time, a network device belongs to only one network namespace. Also, a socket can belong to only one namespace.\nUSer name Space  User namespaces provide a container with its own set of UIDs, \ncompletely separate from those in the parent. So when a process clones a new process it can assign it a new user namespace, as well as a new PID namespace, and all the other namespaces. The cloned process can have access to and privileges for all of the resources of the parent process, or a subset of the resources and privileges of the parent. The user namespaces are considered sensitive in terms of security, as they enable creating non-privileged containers (processes which are created by a non-root user).\nThe LinUx cgro Up SUbSySTem The Linux cgroup subsystem, together with the \nnamespace subsystem, are the basis of lightweight process virtualization, and as such they form the basis of Linux containers; almost every Linux containers project nowadays (such as Docker, LXC, Kubernetes, and others) is based on both of them. The Linux cgroups subsystem provides resource management and accounting. It handles resources such as CPU, network, memory, and more; and it is mostly needed in both ends of the spectrum (embedded devices and servers), and much less in desktops. Development of cgroups was started in 2006 by engineers at Google under the name \u201cprocess containers,\u201d which was later changed to \u201ccgroups\u201d to avoid confusion with Linux Containers. In order to implement cgroups, no new system call was added. A new virtual file system (VFS), \u201ccgroups\u201d (also referred to sometimes as cgroupfs) was added, as all the cgroup filesystem operations are filesystem based. A new version of cgroups, called cgroups v2, was released in kernel 4.5 (March 2016). The cgroup v2 subsystem addressed many of the inconsistencies across cgroup v1 controllers, and made cgroup v2 better organized, by establishing strict and consistent interfaces.\nCurrently, there are 12 cgroup v1 controllers and 3 cgroup v2 controllers (mem-\nory, I/O, and PIDs) and there are other v2 controllers that are a work in progress.\nM04_STAL4290_09_GE_C04.indd   210 5/2/17   4:38 PM\n4.7 / ANDROID PROCESS AND THREAD MANAGEMENT   211\nIn order to use the cgroups filesystem (i.e., browse it, attach tasks to cgroups, \nand so on), it first must be mounted, like when working with any other filesystem. The \ncgroup filesystem can be mounted on any path on the filesystem, and many userspace applications and container projects use /sys/fs/cgroup as a mounting point. After mounting the cgroups filesystem, you can create subgroups, attach processes and tasks to these groups, set limitations on various system resources, and more. The cgroup v1 implementation will probably coexist with the cgroup v2 implementation as long as there are userspace projects that use it; we have a parallel phenomenon in other kernel subsystems, when a new implementation of existing subsystem replaces the current one; for example, currently both iptables and the new nftables coexist, and in the past, iptables coexisted with ipchains.\n 4. 7 ANDR OID PROCESS AND THREAD MANAGEMENT\nBefore discussing the details of the Android approach to process and thread manage-ment, we need to describe the Android concepts of applications and activities.\nAndroid Applications\nAn Android application is the software that implements an app. Each Android appli-cation consists of one or more instance of one or more of four types of application components. Each component performs a distinct role in the overall application behavior, and each component can be activated independently within the applica-tion and even by other applications. The following are the four types of components:\n1.\n A\nctivities: An activity corresponds to a single screen visible as a user interface. \nFor example, an e-mail application might have one activity that shows a list of new e-mails, another activity to compose an e-mail, and another activity for reading e-mails. Although the activities work together to form a cohesive user experience in the e-mail application, each one is independent of the others. Android makes a distinction between internal and exported activities. Other apps may start exported activities, which generally include the main screen of the app. However, other apps cannot start the internal activities. For example, a camera application can start the activity in the e-mail application that composes new mail, in order for the user to share a picture.\n2.\n Services:\n Services are typically used to perform background operations that \ntake a considerable amount of time to finish. This ensures faster responsiveness, for the main thread (a.k.a. UI thread) of an application, with which the user is directly interacting. For example, a service might create a thread to play music in the background while the user is in a different application, or it might create a thread to fetch data over the network without blocking user interaction with an activity. A service may be invoked by an application. Additionally, there are system services that run for the entire lifetime of the Android system, such as Power Manager, Battery, and Vibrator services. These system services create threads that are part of the System Server process.\n3.\n Content pr\noviders: A content provider acts as an interface to application data \nthat can be used by the application. One category of managed data is private \nM04_STAL4290_09_GE_C04.indd   211 5/2/17   4:38 PM\n212  CHAPTER  4 / T HREADS\ndata, which is used only by the application containing the content provider. For \nexample the NotePad application uses a content provider to save notes. The other category is shared data, accessible by multiple applications. This category includes data stored in file systems, an SQLite database, on the Web, or any other persistent storage location your application can access.\n4.\n Br\noadcast receivers: A broadcast receiver responds to system-wide broadcast \nannouncements. A broadcast can originate from another application, such as to let other applications know that some data has been downloaded to the device and is available for them to use, or from the system (for example, a low-battery warning).\nEach application runs on its own dedicated virtual machine and its own single \nprocess that encompasses the application and its virtual machine (see Figure 4.16). \nThis approach, referred to as the sandboxing model, isolates each application. Thus, one application cannot access the resources of the other without permission being granted. Each application is treated as a separate Linux user with its own unique user ID, which is used to set file permissions.\nActivities\nAn Activity is an application component that provides a screen with which users can interact in order to do something, such as make a phone call, take a photo, send an e-mail, or view a map. Each activity is given a window in which to draw its user interface. The window typically fills the screen, but may be smaller than the screen and float on top of other windows.\nAs was mentioned, an application may include multiple activities. When an \napplication is running, one activity is in the foreground, and it is this activity that \nFigure 4.16  Andr oid ApplicationDedicated Process\nBroadcast\nreceiver\nApplication\nDedicated\nvirtual machineContent\nprovider\nActivity Service\nM04_STAL4290_09_GE_C04.indd   212 5/2/17   4:38 PM\n4.7 / ANDROID PROCESS AND THREAD MANAGEMENT   213\ninteracts with the user. The activities are arranged in a last-in-first-out stack (the back \nstack), in the order in which each activity is opened. If the user switches to some other activity within the application, the new activity is created and pushed on to the top of the back stack, while the preceding foreground activity becomes the second item on the stack for this application. This process can be repeated multiple times, adding to the stack. The user can back up to the most recent foreground activity by pressing a Back button or similar interface feature.\nacTiviTy STaTeS Figure 4.17 provides a simplified view of the state transition \ndiagram of an activity. Keep in mind there may be multiple activities in the application, each one at its own particular point on the state transition diagram. When a new activity is launched, the application software performs a series of API calls to the \nFigure 4.17  A ctivity State Transition DiagramResumed\nPausedEntire\nLifetime\nVisible\nLifetime\nStoppedActivity\nlaunched\nApp process\nkilled\nActivity\nshut downonCreate()\nonStart() onRestart()\nonResume()\nonPause()\nonStop()\nonDestroy()User returns\nto the activity\nApps with higher\npriority need memoryUser navigates\nto the activity\nUser navigates\nto the activityForegroundLifetime\nM04_STAL4290_09_GE_C04.indd   213 5/2/17   4:38 PM\n214  CHAPTER  4 / T HREADS\nActivity Manager\u00a0( Figure 2.20 ): onCreate() does the static setup of the activity, \nincluding any data structure initialization; onStart() makes the activity visible to \nthe user on the screen; onResume() passes control to the activity so user input goes to the activity. At this point the activity is in the Resumed state. This is referred to as the foreground lifetime of the activity. During this time, the activity is in front of all \nother activities on screen and has user input focus.\nA user action may invoke another activity within the application. For example, \nduring the execution of the e-mail application, when the user selects an e-mail, a new activity opens to view that e-mail. The system responds to such an activity with the onPause() system call, which places the currently running activity on the stack, putting it in the Paused state. The application then creates a new activity, which will enter the Resumed state.\nAt any time, a user may terminate the currently running activity by means of \nthe Back button, closing a window, or some other action relevant to this activity. The application then invokes onStop(0) to stop the activity. The application then pops the activity that is on the top of the stack and resumes it. The Resumed and Paused states together constitute the visible lifetime of the activity. During this time, the user can see the activity on-screen and interact with it.\nIf the user leaves one application to go to another, for example, by going to the \nHome screen, the currently running activity is paused and then stopped. When the user resumes this application, the stopped activity, which is on top of the back stack, is restarted and becomes the foreground activity for the application.\nKiLLing an appLicaTion If too many things are going on, the system may need to \nrecover some of main memory to maintain responsiveness. In that case, the system will reclaim memory by killing one or more activities within an application and also terminating the process for that application. This frees up memory used to manage the process as well as memory to manage the activities that were killed. However, the application itself still exists. The user is unaware of its altered status. If the user returns to that application, it is necessary for the system to recreate any killed activities as they are invoked.\nThe system kills applications in a stack-oriented style: So it will kill least recently \nused apps first. Apps with foregrounded services are extremely unlikely to be killed.\nProcesses and Threads\nThe default allocation of processes and threads to an application is a single process and a single thread. All of the components of the application run on the single thread of the single process for that application. To avoid slowing down the user interface when slow and/or blocking operations occur in a component, the developer can cre-ate multiple threads within a process and/or multiple processes within an application. In any case, all processes and their threads for a given application execute within the same virtual machine.\nIn order to reclaim memory in a system that is becoming heavily loaded, the sys-\ntem may kill one or more processes. As was discussed in the preceding section, when a process is killed, one or more of the activities supported by that process are also killed. A precedence hierarchy is used to determine which process or processes to kill in order \nM04_STAL4290_09_GE_C04.indd   214 5/2/17   4:38 PM\n4.8 / MAC OS X GRAND CENTRAL DISPATCH   215\nto reclaim needed", "doc_id": "f207099d-ecea-402a-874e-63874d453c3c", "embedding": null, "doc_hash": "3cbddc3b5ba783874ac027c8f31bf4973cd02fbdcd199212ff1f95a3421c7be7", "extra_info": null, "node_info": {"start": 79900, "end": 96584}, "relationships": {"1": "46f5ffc1-aec9-49dc-8b66-426492df7e64", "2": "8ce0d0ef-4e9b-4d84-a745-9abd388b991f", "3": "7a6828b8-65d3-4c6b-ac63-b97e93aa994e"}, "__type__": "1"}, "7a6828b8-65d3-4c6b-ac63-b97e93aa994e": {"text": "/ MAC OS X GRAND CENTRAL DISPATCH   215\nto reclaim needed resources. Every process exists at a particular level of the hierarchy \nat any given time, and processes are killed beginning with the lowest precedence first. The levels of the hierarchy, in descending order of precedence, are as follows:\n\u2022\tForeground process: A process that is required for what the user is currently doing. More than one process at a time can be a foreground process. For exam-ple, both the process that hosts the activity with which the user is interacting (activity in Resumed state), and the process that hosts a service that is bound to the activity with which the user is interacting, are foreground processes.\n\u2022\tVisible process: A process that hosts a component that is not in the foreground, but still visible to the user.\n\u2022\tService process: A process running a service that does not fall into either of the higher categories. Examples include playing music in the background or downloading data on the network.\n\u2022\tBackground process: A process hosting an activity in the Stopped state.\n\u2022\tEmpty process: A process that doesn\u2019t hold any active application components. The only reason to keep this kind of process alive is for caching purposes, to improve startup time the next time a component needs to run in it.\n 4. 8 MA C OS X GRAND CENTRAL DISPATCH\nAs was mentioned in Chapter 2, Mac OS X Grand Central Dispatch (GCD) provides \na pool of available threads. Designers can designate portions of applications, called blocks, that can be dispatched independently and run concurrently. The OS will provide as much concurrency as possible based on the number of cores available and the thread capacity of the system. Although other operating systems have implemented thread pools, GCD provides a qualitative improvement in ease of use and efficiency [LEVI16].\nA block is a simple extension to C or other languages, such as \nC+ +. The pur -\npose of defining a block is to define a self-contained unit of work, including code plus data. Here is a simple example of a block definition:\nx = ^{printf(\u201chello world\\n\u201d);}\nA block is denoted by a caret at the start of the function, which is enclosed in curly brackets. The above block definition defines x  as a way of calling the function, so that \ninvoking the function x() would print the words hello world.\nBlocks enable the programmer to encapsulate complex functions, together with \ntheir arguments and data, so that they can easily be referenced and passed around in a program, much like a variable. Symbolically:\n=  F  +  F data\nBlocks are scheduled and dispatched by means of queues. The application \nmakes use of system queues provided by GCD and may also set up private queues. Blocks are put onto a queue as they are encountered during program execution. GCD then uses those queues to describe concurrency, serialization, and callbacks. Queues are lightweight user-space data structures, which generally makes them far \nM04_STAL4290_09_GE_C04.indd   215 5/2/17   4:38 PM\n216  CHAPTER  4 / T HREADS\nmore efficient than manually managing threads and locks. For example, this queue \nhas three blocks:\nQueueH G F\nDepending on the queue and how it is defined, GCD treats these blocks either \nas potentially concurrent activities, or as serial activities. In either case, blocks are dis-patched on a first-in-first-out basis. If this is a concurrent queue, then the dispatcher assigns F to a thread as soon as one is available, then G, then H. If this is a serial queue, the dispatcher assigns F to a thread, then only assigns G to a thread after F has com-pleted. The use of predefined threads saves the cost of creating a new thread for each request, reducing the latency associated with processing a block. Thread pools are automatically sized by the system to maximize the performance of the applications using GCD while minimizing the number of idle or competing threads.\nH G F\nPool Thread\nIn addition to scheduling blocks directly, the application can associate a single \nblock and queue with an event source, such as a timer, network socket, or file descrip-tor. Every time the source issues an event, the block is scheduled if it is not already running. This allows rapid response without the expense of polling or \u201cparking a thread\u201d on the event source.\nEE Source\nE\nAn example from [SIRA09] indicates the ease of using GCD. Consider a \n d\nocument-based application with a button that, when clicked, will analyze the current \ndocument and display some interesting statistics about it. In the common case, this analysis should execute in under a second, so the following code is used to connect the button with an action:\n- (Inaction)analyzeDocument:(NSButton *)sender\n{   NSDictionary *stats = [myDoc analyze];   [myModel setDict:stats];\nM04_STAL4290_09_GE_C04.indd   216 5/2/17   4:38 PM\n4.9 / SUMMARY   217\n   [myStatsView setNeedsDisplay:YES];\n   [stats release];}\nThe first line of the function body analyzes the document, the second line \nupdates the application\u2019s internal state, and the third line tells the application that the \nstatistics view needs to be updated to reflect this new state. This code, which follows a common pattern, is executed in the main thread. The design is acceptable so long as the analysis does not take too long, because after the user clicks the button, the main thread of the application needs to handle that user input as fast as possible so it can get back to the main event loop to process the next user action. But if the user opens a very large or complex document, the analyze step may take an unacceptably long amount of time. A developer may be reluctant to alter the code to meet this unlikely event, which may involve application-global objects, thread management, callbacks, argument marshalling, context objects, new variables, and so on. But with GCD, a modest addition to the code produces the desired result:\n- (IBAction)analyzeDocument:(NSButton *)sender\n    {di\n spatch_async(dispatch_get_global_queue(0, 0), ^{\n  \n NSDictionary *stats = [myDoc analyze];\n       \ndisp\n \natch_async(dispatch_get_main_queue(), ^{  \n[myModel setDict:stats];\n           [myStatsView setNeedsDisplay:YES];           [stats release];         });  \n     });  }\nAll functions in GCD begin with dispatch_. The outer dispatch_async()  \ncall puts a task on a global concurrent queue. This tells the OS that the block can be \nassigned to a separate concurrent queue, off the main queue, and executed in parallel. Therefore, the main thread of execution is not delayed. When the analyze function is complete, the inner dispatch_async() call is encountered. This directs the OS to put the following block of code at the end of the main queue, to be executed when it reaches the head of the queue. So, with very little work on the part of the program-mer, the desired requirement is met.\n 4.9 SUMMARY\nSome operating systems distinguish the concepts of process and thread, the for -\nmer related to resource ownership, and the latter related to program execution. This approach may lead to improved efficiency and coding convenience. In a multi-threaded system, multiple concurrent threads may be defined within a single process. This may be done using either user-level threads or kernel-level threads. User-level \nM04_STAL4290_09_GE_C04.indd   217 5/2/17   4:38 PM\n218  CHAPTER  4 / T HREADS\nthreads are unknown to the OS and are created and managed by a threads library \nthat runs in the user space of a process. User-level threads are very efficient because a mode switch is not required to switch from one thread to another. However, only a single user-level thread within a process can execute at a time, and if one thread blocks, the entire process is blocked. Kernel-level threads are threads within a process that are maintained by the kernel. Because they are recognized by the kernel, mul-tiple threads within the same process can execute in parallel on a multiprocessor and the blocking of a thread does not block the entire process. However, a mode switch is required to switch from one thread to another.\n  4.10 KEY TERMS, REVIEW Q UESTIONS, AND PROBLEMS\nKey Terms\napplication\nfiberjacketingjob objectkernel-level threadlightweight processmessagemultithreadingnamespacesportprocesstaskthreadthread pooluser-level threaduser-mode scheduling (UMS)\nReview Questions\n 4.1 .  Table 3.5  lists typical elements found in a pr ocess control block for an unthreaded OS. \nOf these, which should belong to a thread control block, and which should belong to \na process control block for a multithreaded system? \n 4.2\n. List reasons why a \nmode switch between threads may be cheaper than a mode switch \nbetween processes.\n 4.3\n. What ar\ne the two separate and potentially independent characteristics embodied in \nthe concept of process?\n 4.4\n. Give four general examples of the use of thr\neads in a single-user multiprocessing \nsystem.\n 4.5\n. How is a thread dif\nferent from a process?\n 4.6\n. What ar\ne the advantages of using multithreading instead of multiple processes?\n 4.7. List some advantages and disadv\nantages of using kernel-level threads.\n 4.8\n. Explain the concept of threads in the case of the Clouds oper\nating system.\nProblems\n 4.1 .  The use of multithreading improves the overall efficiency and performance of the execution of an application or program. However, not all programs are \nsuitable for \nmultithreading. Can you give some examples of programs where a multithreaded solu-tion fails to improve on the performance of a single-threaded solution? Also give some examples where the performance improves when multiple threads are used in place of single threads.\nM04_STAL4290_09_GE_C04.indd   218 5/2/17   4:38 PM", "doc_id": "7a6828b8-65d3-4c6b-ac63-b97e93aa994e", "embedding": null, "doc_hash": "3319029a1eb0fcd272d95863e6f22702f7a1589d3daf05845d3b3fdbcd9e24de", "extra_info": null, "node_info": {"start": 96606, "end": 106299}, "relationships": {"1": "46f5ffc1-aec9-49dc-8b66-426492df7e64", "2": "f207099d-ecea-402a-874e-63874d453c3c"}, "__type__": "1"}}, "ref_doc_info": {"46f5ffc1-aec9-49dc-8b66-426492df7e64": {"doc_hash": "d3c32019d6fb44e83728d6c2b0f875bd48512d632b9e9c5438f702e7d06cbb4f"}, "dc6dfdfb-e39e-44b6-ac60-fc0cc21cf377": {"doc_hash": "fa7e7645fb822fd089cb37009c93e2a9fd51d2287f357050d44d236563da231b"}, "e7033e90-60f5-4941-a7f0-1fb1dea05e60": {"doc_hash": "0ad37ef7af7125ee3ef3adc8a7e6a318f0e6099b2dbb9e03bd42de4ce093df6f"}, "71488cdf-da26-4d3e-b2e4-5d77797ec993": {"doc_hash": "602875c17d199adeb3ca000c1bb76f05c9c6345f1f55165e62bc4f8b16805ff4"}, "c7c63ae5-7725-4b1f-9fcc-62b5991a609b": {"doc_hash": "f973d12a1d762ed2b79bd2fb104c94709b5d98818bce4e28e275a926e2d6b00c"}, "8ce0d0ef-4e9b-4d84-a745-9abd388b991f": {"doc_hash": "f9290d9e387de4cb4895fd8b3095ab541cdccdbef81f5e851a91f3ddb68ff7ed"}, "f207099d-ecea-402a-874e-63874d453c3c": {"doc_hash": "3cbddc3b5ba783874ac027c8f31bf4973cd02fbdcd199212ff1f95a3421c7be7"}, "7a6828b8-65d3-4c6b-ac63-b97e93aa994e": {"doc_hash": "3319029a1eb0fcd272d95863e6f22702f7a1589d3daf05845d3b3fdbcd9e24de"}}}}